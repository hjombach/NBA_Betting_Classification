{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBA_Betting_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UVRldWf9XNuO",
        "8Ex0V_pw7ve2",
        "yo-JLRsV-5J_",
        "ZYWLToAbXD9S",
        "3LhHTeyPBJ7A",
        "qV14p9ThB5tt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVRldWf9XNuO"
      },
      "source": [
        "### Import Libraries and Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFhQiPKWG_3F"
      },
      "source": [
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "import xlrd\n",
        "import openpyxl\n",
        "from openpyxl import load_workbook\n",
        "import os\n",
        "import os.path\n",
        "from os import path\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "pd.options.mode.chained_assignment = None\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEmAXJ_sJExN"
      },
      "source": [
        "os.chdir(\"drive/My Drive\")  # after you run this once you will get an error if you run again. You can ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPp1umWFNNaE"
      },
      "source": [
        "# SET PARAMETERS AND CHOOSE ANALYSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-LiIR4IklS"
      },
      "source": [
        "# Create a new spread threshold to base bet decisions on\n",
        "create_new_threshold = False\n",
        "\n",
        "global favorite_threshold \n",
        "favorite_threshold = 4\n",
        "global underdog_threshold \n",
        "underdog_threshold = -4\n",
        "global bet_amount\n",
        "bet_amount = 10\n",
        "\n",
        "# To restrict the months of analysis\n",
        "restrict_months = False\n",
        "start_month = 2\n",
        "end_month = 10\n",
        "plot_histograms = True\n",
        "plot_confusions = True\n",
        "\n",
        "## CHOOSE DATA TO EXPORT\n",
        "# analysis = 'offensive'\n",
        "# analysis = 'defensive'\n",
        "analysis = 'differential'\n",
        "\n",
        "print_training_output = True\n",
        "print_test_output = True\n",
        "print_cv_output = True\n",
        "plot_feature_importance = True\n",
        "\n",
        "## Choose Models to Run\n",
        "run_logistic = True\n",
        "run_rf_random_search = True\n",
        "run_svc = True\n",
        "run_extra_trees = True\n",
        "run_xgb = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMwdv0AokKto"
      },
      "source": [
        "if create_new_threshold == False:\n",
        "  full_box_scores = pd.read_excel('SML_Term_Project/NBA_Data/Data_Files/Game_Outcomes_and_Odds+{}{}.xlsx'.format(str(favorite_threshold), \n",
        "                                                                                                                 str(underdog_threshold)))\n",
        "  game_outcomes = full_box_scores\n",
        "\n",
        "  combined =  pd.read_excel('SML_Term_Project/NBA_Data/Data_Files/Outcomes_Bets_Team_Differentials_+{}{}.xlsx'.format(str(favorite_threshold), \n",
        "                                                                                                                      str(underdog_threshold)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82d8o0LgNUzY"
      },
      "source": [
        "# EXTRACT ODDS AND SCORES FOR GAMES / LABEL BETTING THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HY3lm0zHlAk"
      },
      "source": [
        "if create_new_threshold:\n",
        "  \n",
        "  seasons = ['2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19', \n",
        "            '2019-20', '2020-21']\n",
        "  \n",
        "  season_end_dates = ['\"2011-04-15\"', '\"2012-04-27\"', '\"2013-04-19\"', '\"2014-04-18\"',\n",
        "                      '\"2015-04-17\"', '\"2016-04-15\"', '\"2017-04-14\"', '\"2018-04-13\"', '\"2019-04-12\"', \n",
        "                      '\"2020-08-17\"', '\"2021-05-21\"']\n",
        "\n",
        "  # remove playoff games\n",
        "  season_end_dates = [dt.datetime.strptime(date, '\"%Y-%m-%d\"').date() for date in season_end_dates]\n",
        "\n",
        "  # create empty dataframe\n",
        "  full_box_scores = pd.DataFrame()\n",
        "\n",
        "  # add each years games to the dataframe. Each game is two lines, one for home team, one for away team. The loop \n",
        "  # splits the home and away teams, and merges the data so they make a single line per game.\n",
        "\n",
        "  year = 2011\n",
        "  i = 0\n",
        "\n",
        "  for season in seasons:\n",
        "\n",
        "      yearly_odds = pd.read_excel(\"SML_Term_Project/NBA_Data/Yearly_Odds/{}.xlsx\".format(season))\n",
        "      visiting_teams = yearly_odds[yearly_odds['VH'] == 'V']\n",
        "      home_teams = yearly_odds[yearly_odds['VH'] == 'H']\n",
        "      yearly_box_scores = home_teams.reset_index().merge(visiting_teams.reset_index(), \n",
        "                                                  left_index=True, right_index=True, how='left')\n",
        "      \n",
        "      # Year is single year. For each season the year is labeled as the year the season ends\n",
        "      yearly_box_scores['Season'] = year\n",
        "      yearly_box_scores['Date_x'] = pd.to_datetime(yearly_box_scores['Date_x']).dt.date\n",
        "      yearly_box_scores = yearly_box_scores[(yearly_box_scores['Date_x'] < season_end_dates[i])]\n",
        "      full_box_scores = full_box_scores.append(yearly_box_scores)\n",
        "\n",
        "      year += 1\n",
        "      i += 1\n",
        "\n",
        "  # Rename columns\n",
        "  full_box_scores.columns = ['index_H','Date','Rot_H','VH_H','Team_H','1st_H','2nd_H','3rd_H', '4th_H','Final_H',\n",
        "                            'Open_H','Close_H','ML_H','2H_H','index_V','Date_V','Rot_V','VH_V','Team_V','1st_V',\n",
        "                            '2nd_V','3rd_V','4th_V','Final_V','Open_V','Close_V','ML_V','2H_V', 'Season']\n",
        "\n",
        "  # Drop unneeded columns\n",
        "  full_box_scores.drop(['index_H', 'Date_V', 'index_V', 'VH_H', 'VH_V'], axis=1)\n",
        "\n",
        "  # Reorder columns\n",
        "  full_box_scores = full_box_scores[['Season', 'Date','Rot_H', 'Team_H','Team_V', '1st_H','2nd_H','3rd_H', '4th_H',\n",
        "                            'Open_H','Close_H','ML_H','2H_H', 'Rot_V','1st_V',\n",
        "                            '2nd_V','3rd_V','4th_V','Open_V','Close_V','ML_V','2H_V', 'Final_H', 'Final_V']]\n",
        "\n",
        "  # Some games indicate a tie. Removing them all for now.\n",
        "  full_box_scores = full_box_scores[full_box_scores['Final_H'] != full_box_scores['Final_V']]\n",
        "\n",
        "  ## The code below allows you to restrict the months of the year that the model predicts the outcome\n",
        "\n",
        "  import datetime as dt\n",
        "\n",
        "  if restrict_months:\n",
        "\n",
        "    full_box_scores['season_date'] = [x[:-4] for x in full_box_scores['Date']]\n",
        "    full_box_scores['Date_month'] = full_box_scores['Date'].astype('datetime64').dt.month\n",
        "    mask = (full_box_scores['Date_month'] >= start_month) & (full_box_scores['Date_month'] < end_month)\n",
        "    full_box_scores = full_box_scores[mask]\n",
        "\n",
        "  betting_lines = full_box_scores [['Open_H', 'Open_V']]\n",
        "\n",
        "  betting_lines = betting_lines.replace('pk', 0) # pk represents 'pick em' which you typically can't bet on\n",
        "  betting_lines = betting_lines.replace('PK', 0) # pk represents 'pick em' which you typically can't bet on\n",
        "\n",
        "  total_points = []\n",
        "  point_spreads = []\n",
        "\n",
        "  # The vegas spread and vegas total points are initially in the same column. \n",
        "  # One is on the line for the home team and the other for the away team. However\n",
        "  # the order is not always consistent. Now that we have separated home and away\n",
        "  # and merged them to be side by side, we will compare the columns and choose\n",
        "  # the lesser of the values to be the spread and the greater value to be the\n",
        "  # total points.\n",
        "\n",
        "  for col1, col2 in zip(betting_lines['Open_H'], betting_lines['Open_V']):\n",
        "\n",
        "    total_points.append(max(col1, col2))\n",
        "    point_spreads.append(min(col1, col2))\n",
        "\n",
        "\n",
        "  full_box_scores['Vegas Total Points'] = total_points\n",
        "  full_box_scores['Vegas Spread'] = point_spreads\n",
        "  full_box_scores['Real Total Points'] = full_box_scores['Final_H'] + full_box_scores['Final_V']\n",
        "\n",
        "\n",
        "  # Vegas sometimes indicates \"Pick 'em\" to indicate they do not have a spread\n",
        "  # to bet on for that game. We have changed these values to 0 and are now \n",
        "  # removing these samples from the data\n",
        "\n",
        "  full_box_scores = full_box_scores[full_box_scores['Vegas Spread'] != 0]\n",
        "\n",
        "  # Drop the now uncecessary columns\n",
        "  full_box_scores = full_box_scores[['Season', 'Date', 'Team_H', 'Team_V', 'ML_H', 'ML_V', 'Final_H', 'Final_V', 'Vegas Total Points', \n",
        "                                     'Real Total Points', 'Vegas Spread']]\n",
        "\n",
        "  # Change values from type 'object' to int\n",
        "  full_box_scores['ML_H'] = full_box_scores['ML_H'].astype(int)\n",
        "  full_box_scores['ML_V'] = full_box_scores['ML_V'].astype(int)\n",
        "\n",
        "\n",
        "  # if the money lines are the same sign, an error has occurred. Remove them\n",
        "  full_box_scores = full_box_scores[full_box_scores['ML_V']*full_box_scores['ML_H'] < 0]\n",
        "\n",
        "  # Add a binary attribute indicating whether the favorite was home or away\n",
        "  full_box_scores.loc[full_box_scores['ML_H'] <0, ['Favorite Home?']] = '1'\n",
        "  full_box_scores.loc[full_box_scores['ML_H'] > 0, ['Favorite Home?']] = '0'\n",
        "\n",
        "  full_box_scores['Favorite_Team'] = np.where(full_box_scores.ML_H < 0, full_box_scores['Team_H'], full_box_scores['Team_V'])\n",
        "  full_box_scores['Underdog_Team'] = np.where(full_box_scores.ML_H > 0, full_box_scores['Team_H'], full_box_scores['Team_V'])\n",
        "\n",
        "  full_box_scores['Favorite_ML'] = np.where(full_box_scores.ML_H < 0, full_box_scores['ML_H'], full_box_scores['ML_V'])\n",
        "  full_box_scores['Underdog_ML'] = np.where(full_box_scores.ML_H > 0, full_box_scores['ML_H'], full_box_scores['ML_V'])\n",
        "\n",
        "  full_box_scores['Favorite_Score'] = np.where(full_box_scores.ML_H < 0, full_box_scores['Final_H'], full_box_scores['Final_V'])\n",
        "  full_box_scores['Underdog_Score'] = np.where(full_box_scores.ML_H < 0, full_box_scores['Final_V'], full_box_scores['Final_H'])\n",
        "\n",
        "  # Drop the now unneeded columns\n",
        "  full_box_scores = full_box_scores.drop(['Team_H', 'Team_V', 'ML_H', 'ML_V', 'Final_H', 'Final_V'], axis=1)\n",
        "\n",
        "\n",
        "  # Calculate the real spread from the game scores and the difference from the vegas predictions\n",
        "  full_box_scores['Real Spread'] = full_box_scores['Favorite_Score'] - full_box_scores['Underdog_Score']\n",
        "  full_box_scores['Spread Difference'] = full_box_scores['Real Spread'] - full_box_scores['Vegas Spread']\n",
        "\n",
        "\n",
        "  # Remove outliers/incorrect observations\n",
        "  full_box_scores = full_box_scores[full_box_scores['Vegas Spread'] > -50]\n",
        "  full_box_scores = full_box_scores[full_box_scores['Vegas Spread'] < 50]\n",
        "\n",
        "  def que(x):\n",
        "      if x['Favorite_Score'] > x['Underdog_Score']:\n",
        "          return 'Favorite won'\n",
        "      if x['Favorite_Score'] < x['Underdog_Score']:\n",
        "        return 'Underdog won'\n",
        "\n",
        "  full_box_scores['Game Outcome'] = full_box_scores.apply(que, axis=1)\n",
        "\n",
        "  fav_covered = full_box_scores[(full_box_scores['Game Outcome'] == 'Favorite won') & (full_box_scores['Spread Difference'] > 0)]\n",
        "\n",
        "  underdog_covered =  full_box_scores.merge(fav_covered, on=['Season','Date', 'Favorite_Team', 'Underdog_Team'], how='left')\n",
        "  underdog_covered = underdog_covered[underdog_covered.isna().any(axis=1)]\n",
        "\n",
        "  underdog_covered = underdog_covered.loc[:, :'Game Outcome_x']\n",
        "\n",
        "  underdog_covered.columns = ['Season', 'Date', 'Vegas Total Points', 'Real Total Points', \n",
        "        'Vegas Spread', 'Favorite Home?', 'Favorite_Team', 'Underdog_Team',\n",
        "        'Favorite_ML', 'Underdog_ML', 'Favorite_Score', 'Underdog_Score', \n",
        "        'Real Spread', 'Spread Difference', 'Game Outcome']\n",
        "\n",
        "\n",
        "  fav_covered['Status'] = 'Favorite covered'\n",
        "  underdog_covered['Status'] = 'Underdog covered'\n",
        "\n",
        "  fav_covered.loc[fav_covered['Spread Difference'] >= favorite_threshold, ['Decision']] = 'Bet Favorite'\n",
        "  fav_covered.loc[fav_covered['Spread Difference'] < favorite_threshold, ['Decision']] = \"Don't Bet\"\n",
        "\n",
        "  underdog_covered.loc[underdog_covered['Spread Difference'] <= underdog_threshold, ['Decision']] = 'Bet Underdog'\n",
        "  underdog_covered.loc[underdog_covered['Spread Difference'] >underdog_threshold, ['Decision']] = \"Don't Bet\"\n",
        "\n",
        "  # Recombine all the data with the target labels\n",
        "\n",
        "  full_box_scores = fav_covered.append(underdog_covered)\n",
        "\n",
        "  full_box_scores = full_box_scores[['Season', 'Date', 'Favorite_Team', 'Underdog_Team', 'Favorite_ML',\n",
        "        'Underdog_ML', 'Favorite_Score', 'Underdog_Score', 'Vegas Total Points', 'Real Total Points',\n",
        "        'Vegas Spread', 'Real Spread', 'Spread Difference',\n",
        "        'Game Outcome', 'Status', 'Decision',\n",
        "        'Favorite Home?']]\n",
        "\n",
        "\n",
        "  full_box_scores.to_excel('SML_Term_Project/NBA_Data/Data_Files/Game_Outcomes_and_Odds+{}{}.xlsx'.format(favorite_threshold, \n",
        "                                                                                                          underdog_threshold), index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ex0V_pw7ve2"
      },
      "source": [
        "# Exploratory Data Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCS0GzRtHlbn"
      },
      "source": [
        "full_box_scores['Vegas Spread'].min()\n",
        "full_box_scores['Vegas Spread'].max()\n",
        "full_box_scores['Vegas Total Points'].min()\n",
        "full_box_scores['Vegas Total Points'].max()\n",
        "\n",
        "full_box_scores.sort_values(by='Vegas Spread', ascending=False) # We should remove Vegas spread < -50\n",
        "full_box_scores.sort_values(by='Vegas Total Points', ascending=False) # We should remove values greater than 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw_Mu8hgIItx"
      },
      "source": [
        "if plot_histograms:\n",
        "  \n",
        "  x = full_box_scores['Vegas Spread']\n",
        "  y = full_box_scores['Real Spread']\n",
        "  z = full_box_scores['Vegas Total Points']\n",
        "  p = full_box_scores['Real Total Points']\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = (14, 12)\n",
        "\n",
        "  fig, axs = plt.subplots(2, 2, sharey=False, tight_layout=True)\n",
        "\n",
        "  # We can set the number of bins with the `bins` kwarg\n",
        "  axs[0][0].hist(x, bins=50)\n",
        "  axs[0][0].set_xlim(0, 25)\n",
        "  axs[0][0].set_title(\"Distribution of Vegas Point Spread\", size=16)\n",
        "  axs[0][1].hist(y, bins=60)\n",
        "  axs[0][1].set_title(\"Distribution of Actual Point Spread\", size=16)\n",
        "\n",
        "  axs[1][0].hist(z, bins=1000)\n",
        "  axs[1][0].set_xlim(140, 280)\n",
        "  axs[1][0].set_title(\"Distribution of Vegas Total Points\", size=16)\n",
        "  axs[1][1].hist(p, bins=100)\n",
        "  axs[1][1].set_title(\"Distribution of Actual Total Points\", size=16)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPmS9j8IIyQ"
      },
      "source": [
        "if plot_histograms:\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,8)\n",
        "\n",
        "  plt.hist(full_box_scores['Spread Difference'], bins =100)\n",
        "  # plt.xlim(0, 1000)\n",
        "  plt.title(\"Distribution of Differences Between Actual Point Spreads and Vegas Predictions \", size=16)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuaRYMYjdMFq"
      },
      "source": [
        "if plot_histograms:\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = (8,6)\n",
        "\n",
        "  n, bins, patches = plt.hist(full_box_scores['Spread Difference'], bins =100, color = 'darkgray')\n",
        "\n",
        "  for i in range(0, 46):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  for i in range(61, 100):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "\n",
        "  # plt.xlim(0, 1000)\n",
        "  plt.xlabel(\"Vegas Predicted Spread Minus the Actual Point Spread\", size=12)\n",
        "  plt.title(\"How Close is the Predicted Spread to the Actual Point Spread?\", size=14)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkPwvvV-oLSf"
      },
      "source": [
        "if plot_histograms:\n",
        "\n",
        "  z = full_box_scores['Spread Difference']\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = (5, 10)\n",
        "\n",
        "  fig, axs = plt.subplots(3, 1, sharey=False, tight_layout=True)\n",
        "\n",
        "  # We can set the number of bins with the `bins` kwarg\n",
        "  n, bins, patches = axs[0].hist(z, bins=100)\n",
        "  axs[0].set_title(\"Games Labeled to Bet with 6 Point Threshold\", size=14)\n",
        "\n",
        "  for i in range(0, 45):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  for i in range(59, 100):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  n, bins, patches =axs[1].hist(z, bins=100)\n",
        "  axs[1].set_title(\"Games Labeled to Bet with 4 Point Threshold\", size=14)\n",
        "  for i in range(0, 47):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  for i in range(57, 100):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  n, bins, patches =axs[2].hist(z, bins=100)\n",
        "\n",
        "\n",
        "  axs[2].set_title(\"Games Labeled to Bet with 2 Point Threshold\", size=14)\n",
        "  for i in range(0, 50):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  for i in range(54, 100):\n",
        "    patches[i].set_fc('orange')\n",
        "\n",
        "  axs[2].set_xlabel(\"Difference Between Predicted and Actual Spread\", size =12)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnSSizXVIJGj"
      },
      "source": [
        "print(\"Frequencies of real game outcomes:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(full_box_scores['Game Outcome'].value_counts())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Frequencies of real game outcomes:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(full_box_scores['Status'].value_counts())\n",
        "print(\"\\n\")\n",
        "\n",
        "# How often does our model indicate we should bet?\n",
        "print(\"The outcome of our calculated decisions:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(full_box_scores.Decision.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efhyl6IXLSpp"
      },
      "source": [
        "# EXTRACT YEARLY SEASON STATS FOR TEAMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCe6rl4s9kVn"
      },
      "source": [
        "### Use the game outcomes and odds to now incorporate the team statistics to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwe6q2-vLbjF"
      },
      "source": [
        "if create_new_threshold:\n",
        "\n",
        "  game_outcomes = full_box_scores\n",
        "\n",
        "  seasons = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "\n",
        "  full_team_statistics = pd.DataFrame()\n",
        "\n",
        "\n",
        "  for season in seasons:\n",
        "\n",
        "      if analysis == 'offensive':\n",
        "        team_stats = ('https://www.espn.com/nba/stats/team/_/season/{}/seasontype/2'.format(season))\n",
        "      elif analysis == 'defensive':\n",
        "        team_stats = ('https://www.espn.com/nba/stats/team/_/view/opponent/season/{}/seasontype/2'.format(season))\n",
        "      elif analysis == 'differential':\n",
        "        team_stats = ('https://www.espn.com/nba/stats/team/_/view/differential/season/{}/seasontype/2'.format(season))\n",
        "      \n",
        "\n",
        "      all_team_data = pd.read_html(team_stats, header=0)\n",
        "\n",
        "      teams = all_team_data[0]\n",
        "      team_data = all_team_data[1] \n",
        "\n",
        "      teams['row_id'] = np.arange(teams.shape[0])\n",
        "      team_data['row_id'] = np.arange(team_data.shape[0])\n",
        "\n",
        "      yearly_stats = pd.merge(teams, team_data, on='row_id', how='outer')\n",
        "      yearly_stats.drop(['row_id', 'GP'], axis =1)\n",
        "\n",
        "      yearly_stats['Season'] = season\n",
        "      full_team_statistics = full_team_statistics.append(yearly_stats)\n",
        "\n",
        "\n",
        "  full_team_statistics = full_team_statistics.drop(['row_id', 'RK', 'GP'], axis=1)\n",
        "\n",
        "  # Update the names of the teams. Needed for future merging with other data files\n",
        "\n",
        "  full_team_statistics = full_team_statistics.replace('Los Angeles Lakers', 'LALakers')\n",
        "  full_team_statistics = full_team_statistics.replace('LA Clippers', 'LAClippers')\n",
        "  full_team_statistics['Team'] = full_team_statistics['Team'].str.rsplit(' ',1).str[0]\n",
        "  full_team_statistics['Team'] = full_team_statistics['Team'].str.replace(' ', '')\n",
        "\n",
        "  full_team_statistics['Season'] = full_team_statistics['Season'].astype(int)\n",
        "\n",
        "\n",
        "  ## Combine the game odds and the team statistics and game odds for the favorites, followed by the team statistics for the underdogs\n",
        "\n",
        "  combined = pd.merge(game_outcomes, full_team_statistics,  how='inner',left_on=['Season', 'Favorite_Team'],right_on=['Season','Team'])\n",
        "\n",
        "  combined.rename(columns={'PTS':'PTS_F','FGM': 'FGM_F','FGA': \"FGA_F\", 'FG%': \"FG%_F\", '3PM':'3PM_F', '3PA': '3PA_F', '3P%': '3P%_F', 'FTM': 'FTM_F',\n",
        "                          'FTA': 'FTA_F', 'FT%': 'FT%_F', 'OR': 'OR_F', 'DR': 'DR_F', 'REB': 'REB_F', 'AST': 'AST_F', 'STL': 'STL_F', 'BLK': 'BLK_F',\n",
        "                          'TO': 'TO_F', 'PF': 'PF_F'\n",
        "                          }, inplace=True)\n",
        "\n",
        "  combined = combined.drop(['Team'], axis=1)\n",
        "\n",
        "  combined = pd.merge(combined, full_team_statistics,  how='inner',left_on=['Season', 'Underdog_Team'],right_on=['Season','Team'])\n",
        "\n",
        "  combined.rename(columns={'PTS':'PTS_U','FGM': 'FGM_U','FGA': \"FGA_U\", 'FG%': \"FG%_U\", '3PM':'3PM_U', '3PA': '3PA_U', '3P%': '3P%_U', 'FTM': 'FTM_U',\n",
        "                          'FTA': 'FTA_U', 'FT%': 'FT%_U', 'OR': 'OR_U', 'DR': 'DR_U', 'REB': 'REB_U', 'AST': 'AST_U', 'STL': 'STL_U', 'BLK': 'BLK_U',\n",
        "                          'TO': 'TO_U', 'PF': 'PF_U'\n",
        "                          }, inplace=True)\n",
        "\n",
        "  combined = combined.drop(['Team'], axis=1)\n",
        "\n",
        "  # Export to excel based on the statistics retrieved\n",
        "\n",
        "  if analysis == 'offensive':\n",
        "    combined.to_excel('SML_Term_Project/NBA_Data/Data_Files/Outcomes_Bets_Team_Offensive_+{}{}.xlsx'.format(favorite_threshold, \n",
        "                                                                                                            underdog_threshold), index = False)\n",
        "  elif analysis == 'defensive':\n",
        "    combined.to_excel('SML_Term_Project/NBA_Data/Data_Files/Outcomes_Bets_Team_Defensive_+{}{}.xlsx'.format(favorite_threshold, \n",
        "                                                                                                                underdog_threshold), index = False)\n",
        "  elif analysis == 'differential':\n",
        "    combined.to_excel('SML_Term_Project/NBA_Data/Data_Files/Outcomes_Bets_Team_Differentials_+{}{}.xlsx'.format(favorite_threshold, \n",
        "                                                                                                                underdog_threshold), index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsd7CY1HLw8B"
      },
      "source": [
        "# PREPARE THE DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5cr-g3eCtnX"
      },
      "source": [
        "## Run a Lasso Regression to determine which features to keep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff9dYjFCg7iI"
      },
      "source": [
        "# Lasso CV \n",
        "\n",
        "df = combined\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "df = df.drop(['Season', 'Date', 'Favorite_Team', 'Underdog_Team', \n",
        "              'Favorite_Score', 'Underdog_Score', 'Favorite_ML', 'Underdog_ML',\n",
        "              'Favorite_ML','Underdog_ML', 'Vegas Total Points', \n",
        "              'Real Total Points', 'Real Spread', 'Game Outcome', 'Status', \n",
        "              'Decision'], axis=1)\n",
        "\n",
        "df = df.rename(columns={\"Favorite Home?\": \"Home_F\"})\n",
        "\n",
        "x = df.drop(['Spread Difference'],axis=1)\n",
        "y = df['Spread Difference']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "ct = ColumnTransformer([\n",
        "        ('somename', StandardScaler(), ['Vegas Spread', 'PTS_F', 'FGM_F', 'FGA_F', 'FG%_F',\n",
        "       '3PM_F', '3PA_F', '3P%_F', 'FTM_F', 'FTA_F', 'FT%_F', 'OR_F', 'DR_F',\n",
        "       'REB_F', 'AST_F', 'STL_F', 'BLK_F', 'TO_F', 'PF_F', 'PTS_U', 'FGM_U',\n",
        "       'FGA_U', 'FG%_U', '3PM_U', '3PA_U', '3P%_U', 'FTM_U', 'FTA_U', 'FT%_U',\n",
        "       'OR_U', 'DR_U', 'REB_U', 'AST_U', 'STL_U', 'BLK_U', 'TO_U', 'PF_U'])\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "\n",
        "x_scaled = ct.fit_transform(x_train)\n",
        "x_test_scaled = ct.transform(x_test)\n",
        "\n",
        "x_scaled = pd.DataFrame(x_scaled, columns = x_train.columns)\n",
        "x_test_scaled =  pd.DataFrame(x_test_scaled, columns = x_train.columns)\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "clf = linear_model.Lasso(alpha=0.1).fit(x_scaled, y_train)\n",
        "\n",
        "for i in range(len(clf.coef_)):\n",
        "  if abs(clf.coef_[i]) > 0.01:\n",
        "    print(\"{} = {:.4} \".format(x_train.columns[i], str(clf.coef_[i])))\n",
        "\n",
        "print(\"\\nLasso Intercept: {:.4}\".format(clf.intercept_))\n",
        "\n",
        "### From Lasso - \n",
        "# df = df[['Spread Difference', 'Vegas Spread', 'Home_F', '3PA_F', 'OR_F', 'DR_F', 'PF_F', \n",
        "#         'DR_U', 'STL_U', 'PF_U']]\n",
        "\n",
        "\n",
        "# df.to_excel('SML_Term_Project/NBA_Data/Data_Files/Lasso_Dataframe.xlsx', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zENweFbLLxaK"
      },
      "source": [
        "# Revert data back prior to splitting for Lasso\n",
        "\n",
        "global df_copy\n",
        "df = combined\n",
        "df_copy = df\n",
        "df_copy = df_copy[['Season', 'Date', 'Favorite_Team', 'Underdog_Team', \n",
        "                   'Favorite_ML', 'Underdog_ML', 'Status', 'Decision']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kPdOCUoLxfN"
      },
      "source": [
        "df = df.drop(['Season', 'Date', 'Favorite_Team', 'Underdog_Team', 'Favorite_Score', \n",
        "              'Underdog_Score', 'Favorite_ML', 'Underdog_ML', 'Favorite_ML',\n",
        "              'Underdog_ML', 'Vegas Total Points', 'Real Total Points',\n",
        "              'Real Spread', 'Spread Difference', 'Game Outcome', 'Status'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M4z8YhWir1E"
      },
      "source": [
        "df = df[['Decision', 'Vegas Spread', 'PTS_F', 'FGM_F', 'FGA_F', 'FG%_F',\n",
        "       '3PM_F', '3PA_F', '3P%_F', 'FTM_F', 'FTA_F', 'FT%_F', 'OR_F', 'DR_F',\n",
        "       'REB_F', 'AST_F', 'STL_F', 'BLK_F', 'TO_F', 'PF_F', 'PTS_U', 'FGM_U',\n",
        "       'FGA_U', 'FG%_U', '3PM_U', '3PA_U', '3P%_U', 'FTM_U', 'FTA_U', 'FT%_U',\n",
        "       'OR_U', 'DR_U', 'REB_U', 'AST_U', 'STL_U', 'BLK_U', 'TO_U', 'PF_U', \n",
        "       'Favorite Home?']]\n",
        "\n",
        "df = df.rename(columns={\"Favorite Home?\": \"Home_F\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-WscioODTuB"
      },
      "source": [
        "## Label the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU0-hf9IMT9p"
      },
      "source": [
        "df = df.replace(\"Don't Bet\", 0)\n",
        "df = df.replace('Bet Underdog', 1)\n",
        "df = df.replace('Bet Favorite', 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo-JLRsV-5J_"
      },
      "source": [
        "### Split the Data: Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orSm0-xcMUFT"
      },
      "source": [
        "x = df.drop(['Decision'],axis=1)\n",
        "y = df['Decision']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejIb1SpV-9sa"
      },
      "source": [
        "### Standardize the features (except for the binary feature \"Favorite Home?\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjs_Jg-MUNC"
      },
      "source": [
        "ct = ColumnTransformer([\n",
        "        ('somename', StandardScaler(), ['Vegas Spread', 'PTS_F', 'FGM_F', 'FGA_F', 'FG%_F',\n",
        "       '3PM_F', '3PA_F', '3P%_F', 'FTM_F', 'FTA_F', 'FT%_F', 'OR_F', 'DR_F',\n",
        "       'REB_F', 'AST_F', 'STL_F', 'BLK_F', 'TO_F', 'PF_F', 'PTS_U', 'FGM_U',\n",
        "       'FGA_U', 'FG%_U', '3PM_U', '3PA_U', '3P%_U', 'FTM_U', 'FTA_U', 'FT%_U',\n",
        "       'OR_U', 'DR_U', 'REB_U', 'AST_U', 'STL_U', 'BLK_U', 'TO_U', 'PF_U'])\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "x_scaled = ct.fit_transform(x_train)\n",
        "x_test_scaled = ct.transform(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2he3K2t0I1J"
      },
      "source": [
        "# place back into dataframes. Easier to visualize onwards\n",
        "\n",
        "x_scaled = pd.DataFrame(x_scaled, columns = x_train.columns)\n",
        "x_test_scaled =  pd.DataFrame(x_test_scaled, columns = x_train.columns)\n",
        "\n",
        "# Subset data based on Lasso Regression\n",
        "x_scaled = x_scaled[['Vegas Spread', 'Home_F', '3PA_F', 'OR_F', 'DR_F', 'PF_F', \n",
        "         'DR_U', 'STL_U', 'PF_U']]\n",
        "\n",
        "x_test_scaled = x_test_scaled[['Vegas Spread', 'Home_F', '3PA_F', 'OR_F', 'DR_F', 'PF_F', \n",
        "         'DR_U', 'STL_U', 'PF_U']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYWLToAbXD9S"
      },
      "source": [
        "# Print/Save/Create Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODAmu447MjDQ"
      },
      "source": [
        "# define scoring metrics. See code above for options to choose from\n",
        "cv_scoring = ('balanced_accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted')\n",
        "\n",
        "def print_and_save_output(which_samples, model_type, x_train, y_train_true, y_train_pred, x_test, y_test_true, y_test_pred, cv_results = None):\n",
        "\n",
        "  training_acc = accuracy_score(y_train_true, y_train_pred, normalize=True)\n",
        "  training_weighted_recall = metrics.recall_score(y_train_true, y_train_pred, average = 'weighted')\n",
        "  training_weighted_precision = metrics.precision_score(y_train_true, y_train_pred, average = 'weighted')\n",
        "  training_f1 = metrics.f1_score(y_train_true, y_train_pred, average = 'weighted')\n",
        "\n",
        "  testing_acc = accuracy_score(y_test_true, y_test_pred, normalize=True)\n",
        "  testing_weighted_precision = metrics.precision_score(y_test_true, y_test_pred, average = 'weighted')\n",
        "  testing_weighted_recall = metrics.recall_score(y_test_true, y_test_pred, average = 'weighted')\n",
        "  testing_f1 = metrics.f1_score(y_test_true, y_test_pred, average = 'weighted')\n",
        "\n",
        "\n",
        "  if print_training_output:\n",
        "    print(\"TRAINING RESULTS\")\n",
        "    print(\"-----------------\")\n",
        "    print(\"Model Accuracy: {:.3}\".format(training_acc))\n",
        "    print(\"Weighted recall score: {:.3}\".format(training_weighted_recall))\n",
        "    print(\"Weightedprecision score: {:.3}\".format(training_weighted_precision))\n",
        "    print(\"Weighted f1 score: {:.3}\\n\".format(training_f1))\n",
        "  \n",
        "  if print_cv_output:\n",
        "    print('\\nCROSS VALIDATION RESULTS')\n",
        "    print(\"-------------------------\")\n",
        "    print(\"CV Test Accuracy \", round(cv_results['test_balanced_accuracy'].mean(), 3))\n",
        "    print(\"CV Test Recall Weighted \", round(cv_results['test_recall_weighted'].mean(), 3))\n",
        "    print(\"CV Test Precision Weighted \", round(cv_results['test_precision_weighted'].mean(), 3))    \n",
        "    print(\"CV Test F1 Weighted \", round(cv_results['test_f1_weighted'].mean(), 3))\n",
        "\n",
        "\n",
        "    # for measure in sorted(cv_results.keys()):\n",
        "    #   print(measure, round(cv_results[measure].mean(), 3))\n",
        "  \n",
        "  if print_test_output:\n",
        "    print(\"\\n\\nTEST RESULTS\")\n",
        "    print(\"-----------------\")\n",
        "    print(\"Model Accuracy: {:.4}\".format(testing_acc))\n",
        "    print(\"Weighted-average recall score: {:.4}\".format(testing_weighted_recall))\n",
        "    print(\"Weighted-average precision score: {:.4}\".format(testing_weighted_precision))\n",
        "    print(\"Weighted-average f1 score: {:.4}\".format(testing_f1))\n",
        "\n",
        "  results_to_save = [model_type, favorite_threshold, underdog_threshold, training_acc, training_weighted_precision, training_weighted_recall, \n",
        "                     cv_results['train_f1_weighted'].mean(), cv_results['test_balanced_accuracy'].mean(), \n",
        "                     cv_results['test_precision_weighted'].mean(), cv_results['test_recall_weighted'].mean(),\n",
        "                     cv_results['test_f1_weighted'].mean(), testing_acc, testing_weighted_precision, testing_weighted_recall, testing_f1]\n",
        "\n",
        "############################################################\n",
        "\n",
        "  output_columns = ['Model', 'Favorite Threshold', 'Underdog Threshold', 'Train Accuracy', 'Train Weighted Precision', 'Train Weighted Recall', \n",
        "                    'Train F1', 'CV Accuracy', 'CV Precision', 'CV Recall', 'CV F1', 'Test Acc', 'Test Precision', 'Test Recall', 'Test F1']\n",
        "           \n",
        "  model_outputs = pd.DataFrame(columns = output_columns)\n",
        "\n",
        "  temp = pd.DataFrame([results_to_save], columns=output_columns)\n",
        "  model_outputs = pd.concat([temp, model_outputs])\n",
        "  model_outputs['Analysis'] = analysis\n",
        "\n",
        "############################################################\n",
        "\n",
        "  global betting_file\n",
        "\n",
        "  ## TESTING \n",
        "  if (which_samples == 'Test'):\n",
        "    y_test_df = pd.DataFrame(y_test_true)\n",
        "    y_test_df['Prediction'] = y_test_pred\n",
        "    \n",
        "    testing_labels_preds = x_test.join(y_test_df)\n",
        "    testing_labels_preds = testing_labels_preds[['Decision', 'Prediction']]\n",
        "    testing_labels_preds = testing_labels_preds.sort_index()\n",
        "\n",
        "    betting_file_TESTING_DATA = df_copy.join(testing_labels_preds, lsuffix='_l', rsuffix='_r')\n",
        "    betting_file_TESTING_DATA = betting_file_TESTING_DATA.dropna()\n",
        "    betting_file_TESTING_DATA['Model'] = model_type\n",
        "    betting_file = betting_file_TESTING_DATA\n",
        "\n",
        "  betting_data = betting_file\n",
        "  betting_data = betting_data.drop(['Date'], axis=1)\n",
        "  test_set_games = len(betting_data)\n",
        "\n",
        "  betting_data = betting_data[betting_data.Prediction != 0.0]\n",
        "  betting_data['Prediction'] = betting_data['Prediction'].replace(1.0, 'Underdog covered')\n",
        "  betting_data['Prediction'] = betting_data['Prediction'].replace(2.0, 'Favorite covered')\n",
        "\n",
        "  betting_data['Favorite_Payout'] = round(bet_amount/(betting_data['Favorite_ML']/-100) + bet_amount, 2)\n",
        "  betting_data['Underdog_Payout'] = round(bet_amount*(betting_data['Underdog_ML']/100) + bet_amount, 2)\n",
        "\n",
        "\n",
        "  betting_data['Underdog_My_Payout'] = np.where(((betting_data.Status == 'Underdog covered') & (betting_data.Status == betting_data.Prediction)), \n",
        "                                     betting_data['Underdog_Payout'], -bet_amount)\n",
        "  \n",
        "\n",
        "  betting_data['Favorite_My_Payout'] = np.where(((betting_data.Status == 'Favorite covered') & (betting_data.Status == betting_data.Prediction)), \n",
        "                                      betting_data['Favorite_Payout'], -bet_amount)\n",
        "\n",
        "  betting_data['My_Payout'] = np.where((betting_data.Favorite_My_Payout >-9), \n",
        "                                      betting_data['Favorite_My_Payout'], betting_data['Underdog_My_Payout'])\n",
        "\n",
        "  # ##############\n",
        "  # writer = pd.ExcelWriter('SML_Term_Project/NBA_Data/Model_Outputs/Betting_Output3.xlsx', engine='xlsxwriter')\n",
        "  # betting_data.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "  # writer.save()\n",
        "  # ##############\n",
        "\n",
        "  betting_columns = ['Model_Type', 'Num_Games_Bet', 'Payout', '% Games Bet On', 'Avg $ Lost per Bet', 'Games in Test Set']\n",
        "  percent_bet = len(betting_data)/test_set_games*100\n",
        "  avg_lost = betting_data['My_Payout'].sum()/(len(betting_data))\n",
        "  betting_info = [model_type, len(betting_data), betting_data['My_Payout'].sum(), percent_bet, avg_lost, test_set_games]    \n",
        "\n",
        "  bet_df = pd.DataFrame([betting_info], columns = betting_columns)\n",
        "\n",
        "  model_outputs = model_outputs.join(bet_df)\n",
        "  model_outputs['Date-Time'] = datetime.now().strftime(\"%d/%m/%Y %H:%M\")\n",
        "  model_outputs['Avg. Net Profit per Bet'] = (model_outputs['Avg $ Lost per Bet'] - bet_amount)\n",
        "  model_outputs['Net Payout'] = (model_outputs['Payout'] - model_outputs['Num_Games_Bet']*bet_amount)\n",
        "\n",
        "  ## Export Data to Excel Files\n",
        "  if path.exists(\"SML_Term_Project/NBA_Data/Model_Outputs/Spread_Model_Outputs_Copy.xlsx\"):\n",
        "    writer = pd.ExcelWriter(\"SML_Term_Project/NBA_Data/Model_Outputs/Spread_Model_Outputs_Copy.xlsx\", engine='openpyxl')\n",
        "    writer.book = load_workbook(\"SML_Term_Project/NBA_Data/Model_Outputs/Spread_Model_Outputs_Copy.xlsx\")\n",
        "    writer.sheets = dict((ws.title, ws) for ws in writer.book.worksheets)\n",
        "    reader = pd.read_excel(r\"SML_Term_Project/NBA_Data/Model_Outputs/Spread_Model_Outputs_Copy.xlsx\")\n",
        "    model_outputs.to_excel(writer,index=False,header=False,startrow=len(reader)+1)\n",
        "    writer.close()\n",
        "  else:\n",
        "    writer = pd.ExcelWriter('SML_Term_Project/NBA_Data/Model_Outputs/Spread_Model_Outputs_Copy.xlsx', engine='xlsxwriter')\n",
        "    model_outputs.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "    writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4423CMwW_f5"
      },
      "source": [
        "# RUN THE MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkSfPmAyBhGN"
      },
      "source": [
        "## Basic Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dug5JkPWB0qs"
      },
      "source": [
        "# Trying to determine the most important features\n",
        "\n",
        "log_reg = LogisticRegression(solver = 'liblinear').fit(x_scaled, y_train)\n",
        "\n",
        "y_train_predict = log_reg.predict(x_scaled)\n",
        "y_test_predict = log_reg.predict(x_test_scaled)\n",
        "\n",
        "log_reg_cv_results = None\n",
        "log_reg_cv_results = cross_validate(log_reg, x_scaled, y_train, cv=5, return_train_score=True, scoring = cv_scoring)\n",
        "\n",
        "print_and_save_output(\"Test\", \"Logistic Regression\", x_scaled, y_train, y_train_predict, x_test, y_test, y_test_predict, log_reg_cv_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9_CdRQFqru"
      },
      "source": [
        "## Randomized CV Search for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I2amDfcNvEw"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "\n",
        "if run_logistic:\n",
        "  log_basic = LogisticRegression(solver = 'liblinear')\n",
        "\n",
        "  distributions = dict(C=uniform(loc=0, scale=4),\n",
        "                      penalty=['l2', 'l1'])\n",
        "\n",
        "  log_random = RandomizedSearchCV(log_basic, distributions, random_state=0).fit(x_scaled, y_train)\n",
        "  log_random.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeJgXtSIBm54"
      },
      "source": [
        "## Optimal Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7QpqQkmMjPD"
      },
      "source": [
        "if run_logistic:\n",
        "  \n",
        "  # This custom logistic does not perform any better than the generic one above\n",
        "  log_reg = LogisticRegression(solver='liblinear', C=3.77, penalty='l1').fit(x_scaled, y_train)\n",
        "\n",
        "  y_train_predict = log_reg.predict(x_scaled)\n",
        "  y_test_predict = log_reg.predict(x_test_scaled)\n",
        "\n",
        "  log_reg_cv_results = None\n",
        "  log_reg_cv_results = cross_validate(log_reg, x_scaled, y_train, cv=5, return_train_score=True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"Logistic Regression - Optimized\", x_scaled, y_train, y_train_predict, x_test, y_test, y_test_predict, log_reg_cv_results)\n",
        "  \n",
        "  if plot_confusions:\n",
        "    plot_confusion_matrix(log_reg, x_scaled, y_train, display_labels=[\"Don't Bet\", \"Bet Underdog\", \"Bet Favorite\"], normalize=None, values_format=\"\")  \n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4amyIdmA9Ii"
      },
      "source": [
        "## Random Forest Randomized Search - Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW6azEIzpSgP"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from pprint import pprint\n",
        "\n",
        "if run_rf_random_search:\n",
        "\n",
        "  n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
        "  max_features = ['auto', 'sqrt']\n",
        "  max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "  max_depth.append(None)\n",
        "  min_samples_split = [4, 10, 12]\n",
        "  min_samples_leaf = [1, 2, 4]\n",
        "  bootstrap = [True, False]\n",
        "\n",
        "  random_grid = {'n_estimators': n_estimators,\n",
        "                'max_features': max_features,\n",
        "                'max_depth': max_depth,\n",
        "                'min_samples_split': min_samples_split,\n",
        "                'min_samples_leaf': min_samples_leaf,\n",
        "                'bootstrap': bootstrap}\n",
        "\n",
        "  # pprint(random_grid)\n",
        "\n",
        "  rf = RandomForestClassifier()\n",
        "  rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "  rf_random.fit(x_scaled, y_train)\n",
        "  # print(rf_random.best_params_)\n",
        "\n",
        "  # # Random Forest Model\n",
        "  clf = RandomForestClassifier(max_depth = rf_random.best_params_['max_depth'], \n",
        "                                n_estimators = rf_random.best_params_['n_estimators'], \n",
        "                                min_samples_split = rf_random.best_params_['min_samples_split'],\n",
        "                                min_samples_leaf = rf_random.best_params_['min_samples_leaf'], \n",
        "                                random_state=0).fit(x_scaled, y_train) \n",
        "\n",
        "  rf_train_predictions = clf.predict(x_scaled)\n",
        "  rf_test_predictions = clf.predict(x_test_scaled)\n",
        "\n",
        "  cv_results = None\n",
        "  cv_results = cross_validate(clf, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"RF-Optimal\", x_scaled, y_train, rf_train_predictions, x_test, y_test, rf_test_predictions, cv_results)\n",
        "\n",
        "  if plot_confusions:\n",
        "    plot_confusion_matrix(clf, x_scaled, y_train, display_labels=[\"Don't Bet\", \"Bet Underdog\", \"Bet Favorite\"], normalize=None, values_format=\"\")  \n",
        "    plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LhHTeyPBJ7A"
      },
      "source": [
        "## Random Forest - Optimal Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYNdOctjsVuC"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# # Random Forest Model\n",
        "global clf_3\n",
        "\n",
        "if run_random_forest_3:\n",
        "  clf_3 = RandomForestClassifier(max_depth = rf_random.best_params_['max_depth'], \n",
        "                                 n_estimators = rf_random.best_params_['n_estimators'], \n",
        "                                 min_samples_split = rf_random.best_params_['min_samples_split'],\n",
        "                                 min_samples_leaf = rf_random.best_params_['min_samples_leaf'], \n",
        "                                 random_state=0).fit(x_scaled, y_train) \n",
        "\n",
        "  rf_3_train_predictions = clf_3.predict(x_scaled)\n",
        "  rf_3_test_predictions = clf_3.predict(x_test_scaled)\n",
        "\n",
        "  cv_results_3 = None\n",
        "  cv_results_3 = cross_validate(clf_3, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"RF3\", x_scaled, y_train, rf_3_train_predictions, x_test, y_test, rf_3_test_predictions, cv_results_3)\n",
        "\n",
        "  if plot_confusions:\n",
        "    plot_confusion_matrix(clf_3, x_scaled, y_train, display_labels=[\"Don't Bet\", \"Bet Underdog\", \"Bet Favorite\"], normalize=None, values_format=\"\")  \n",
        "    plt.show()  \n",
        "\n",
        "  if plot_feature_importance:\n",
        "\n",
        "    importance = clf_3.feature_importances_\n",
        "  #   # summarize feature importance\n",
        "  #   for i,v in enumerate(importance):\n",
        "  #   \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "  #  #  plot feature importance\n",
        "  #   plt.bar([x for x in range(len(importance))], importance)\n",
        "  #   plt.show()\n",
        "\n",
        "    print(importance)\n",
        "\n",
        "    top_indeces = sorted(range(len(importance)), key=lambda i: importance[i])[-8:]\n",
        "\n",
        "    top_vals = [importance[i] for i in top_indeces]\n",
        "    top_features = [x_scaled.columns[i] for i in top_indeces]\n",
        "\n",
        "    print(top_vals)\n",
        "    print(top_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV14p9ThB5tt"
      },
      "source": [
        "## Basic SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVKnm1tICHsK"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "if run_svc:\n",
        "\n",
        "  svc_c = SVC().fit(x_scaled, y_train)\n",
        "\n",
        "  svc_train_predicted = svc_c.predict(x_scaled)\n",
        "  svc_test_predicted = svc_c.predict(x_test_scaled)\n",
        "  \n",
        "  svc_cv = None\n",
        "  svc_cv = cross_validate(svc_c, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"SVC - Optimal\", x_scaled, y_train, svc_train_predicted, x_test, y_test, svc_test_predicted, svc_cv)\n",
        "\n",
        "  if plot_confusions:\n",
        "    plot_confusion_matrix(svc_c, x_scaled, y_train, display_labels=[\"Don't Bet\", \"Bet Underdog\", \"Bet Favorite\"], normalize=None, values_format=\"\")  \n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFNYTK8YB4T7"
      },
      "source": [
        "## SVC Randomized SearchCV for Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm2pCQoB8Umx"
      },
      "source": [
        "from scipy import stats\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "if run_svc:\n",
        "\n",
        "  parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100], \n",
        "  \"gamma\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "  search = GridSearchCV(SVC(), parameters, cv=5)\n",
        "  search.fit(x_scaled, y_train)\n",
        "\n",
        "  # print(search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh4UQpI_CddW"
      },
      "source": [
        "## SVC with Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G1ue0JOOCzx"
      },
      "source": [
        "if run_svc:\n",
        "\n",
        "  svc_c = SVC(C=search.best_params_['C'], gamma=search.best_params_['gamma']).fit(x_scaled, y_train)\n",
        "\n",
        "  svc_train_predicted = svc_c.predict(x_scaled)\n",
        "  svc_test_predicted = svc_c.predict(x_test_scaled)\n",
        "  \n",
        "  svc_cv = None\n",
        "  svc_cv = cross_validate(svc_c, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"SVC - Optimal\", x_scaled, y_train, svc_train_predicted, x_test, y_test, svc_test_predicted, svc_cv)\n",
        "\n",
        "  if plot_confusions:\n",
        "    plot_confusion_matrix(svc_c, x_scaled, y_train, display_labels=[\"Don't Bet\", \"Bet Underdog\", \"Bet Favorite\"], normalize=None, values_format=\"\")  \n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQqC6sY3CC6J"
      },
      "source": [
        "## Extra Trees Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WO5T9C8gtLJ"
      },
      "source": [
        "# https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "if run_extra_trees:\n",
        "\n",
        "  extra_class = ExtraTreesClassifier().fit(x_scaled, y_train)\n",
        "\n",
        "  extra_train_predicted = extra_class.predict(x_scaled)\n",
        "  extra_test_predicted = extra_class.predict(x_test_scaled)\n",
        "\n",
        "  extra_cv = cross_validate(extra_class, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"Extra Trees\", x_scaled, y_train, extra_train_predicted, x_test, y_test, extra_test_predicted, extra_cv)\n",
        "  # print(extra_class.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "\n",
        "\n",
        "  #plot graph of feature importances for better visualization\n",
        "  if plot_feature_importance:\n",
        "    feat_importances = pd.Series(extra_class.feature_importances_, index=x_scaled.columns)\n",
        "    feat_importances.nlargest(8).plot(kind='bar')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys78HAtBCFwb"
      },
      "source": [
        "## XGB Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2haYLUxOyxqV"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "if run_xgb:\n",
        "\n",
        "  from sklearn import preprocessing\n",
        "  lbl = preprocessing.LabelEncoder()\n",
        "\n",
        "  cols = x_scaled.select_dtypes(exclude=['string']).columns\n",
        "\n",
        "  x_scaled[cols] = x_scaled[cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
        "  x_test_scaled[cols] = x_test_scaled[cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
        "\n",
        "  x_scaled['Home_F'] = lbl.fit_transform(x_scaled['Home_F'].astype(int))\n",
        "  x_test_scaled['Home_F'] = lbl.fit_transform(x_test_scaled['Home_F'].astype(int))\n",
        "  x_scaled['Home_F'] = x_scaled['Home_F'].astype(bool)\n",
        "  x_test_scaled['Home_F'] = x_test_scaled['Home_F'].astype(bool)\n",
        "\n",
        "  xgb_classifier = XGBClassifier().fit(x_scaled, y_train)\n",
        "\n",
        "  xgb_train_predicted = xgb_classifier.predict(x_scaled)\n",
        "  xgb_test_predicted = xgb_classifier.predict(x_test_scaled)\n",
        "\n",
        "  xgb_cv = None\n",
        "  xgb_cv = cross_validate(xgb_classifier, x_scaled, y_train, cv=5, return_train_score= True, scoring = cv_scoring)\n",
        "\n",
        "  print_and_save_output(\"Test\", \"XGB Boost\", x_scaled, y_train, xgb_train_predicted, x_test, y_test, xgb_test_predicted, xgb_cv)\n",
        " \n",
        "\n",
        "  if plot_feature_importance:\n",
        "    # get importance\n",
        "    importance = xgb_classifier.feature_importances_\n",
        "\n",
        "    plt.bar([x for x in range(len(importance))], importance)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}